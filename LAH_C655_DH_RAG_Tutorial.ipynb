{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1TlyyTfUeD9LGN9elCYkkuaY_qVy9N3m_",
      "authorship_tag": "ABX9TyNzs5mVweRRTrUXBlDwOIgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idiig/lah-c655-dh-rag-demo-2025/blob/master/LAH_C655_DH_RAG_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "---\n",
        "title: LAH.C655 Building a RAG for querying humanities knowledge\n",
        "author:\n",
        "  - name: Xudong Chen\n",
        "    corresponding: false\n",
        "    roles: []\n",
        "    id: jc\n",
        "    orcid: 0000-0002-4542-2878\n",
        "    email: xchen@shs.ens.titech.ac.jp\n",
        "    affiliation:\n",
        "      - name: Tokyo Institute of Technology\n",
        "        city: Tokyo\n",
        "        country: Japan\n",
        "        url: 'https://www.titech.ac.jp/english/'\n",
        "        isni: 0000000121792105\n",
        "        ror: 0112mx960\n",
        "  - name: Hilofumi Yamamoto\n",
        "    corresponding: true\n",
        "    roles: []\n",
        "    id: jc\n",
        "    orcid: 0000-0001-6876-139X\n",
        "    email: yamagen@lia.titech.ac.jp\n",
        "    affiliation:\n",
        "      - name: Tokyo Institute of Technology\n",
        "        city: Tokyo\n",
        "        country: Japan\n",
        "        url: 'https://www.titech.ac.jp/english/'\n",
        "        isni: 0000000121792105\n",
        "        ror: 0112mx960\n",
        "date: 2024/10/10\n",
        "abstract: |\n",
        "  A Retrieval-Augmented Generation (RAG) system for querying humanities knowledge.\n",
        "keywords:\n",
        "  - RAG\n",
        "  - Semiotics\n",
        "  - Digital Humanities\n",
        "license: CC BY\n",
        "copyright:\n",
        "  holder: 'Xudong Chen, Hilofumi Yamamoto'\n",
        "  year: 2024\n",
        "funding: ''\n",
        "format:\n",
        "  html:\n",
        "    code-links:\n",
        "      - text: Data Import Code\n",
        "        icon: file-code\n",
        "        href: 'https://gist.github.com/idiig/d62a55511c9d3e49e9a0f64945f490b4'\n",
        "    theme: default\n",
        "    toc: true\n",
        "    toc-title: Table of Contents\n",
        "    toc-location: right-body\n",
        "    number-sections: true\n",
        "    html-math-method: katex\n",
        "    fig_caption: true\n",
        "    cap-location: margin\n",
        "    reference-location: margin\n",
        "    citation-location: document\n",
        "    code-fold: false\n",
        "    fig-format: svg\n",
        "    fig-width: 6\n",
        "    fig-height: 3.71\n",
        "execute:\n",
        "  echo: false\n",
        "  freeze: auto\n",
        "  message: false\n",
        "crossref:\n",
        "  fig-title: Figure\n",
        "  tbl-title: Table\n",
        "  title-delim: ':\\quad'\n",
        "  fig-prefix: Figure\n",
        "  tbl-prefix: Table\n",
        "  sec-prefix: Section\n",
        "  eq-prefix: Eq.\n",
        "jupyter:\n",
        "  kernel: \"python3\"  # Specify the kernel for the notebook\n",
        "---"
      ],
      "metadata": {
        "id": "d-ciiJF2j6uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAH.C655 Building a RAG for querying humanities knowledge"
      ],
      "metadata": {
        "id": "Yx3JFwuQ-QlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this class, we will be building a **RAG system** using **humanities data**.\n",
        "\n",
        "Here’s what we’ll cover:\n",
        "\n",
        "1. We’ll start with the basics of RAG.\n",
        "2. Then, we’ll explore resources related to the humanities field.\n",
        "3. After that, we’ll discuss the type of RAG system you'd like to create.\n",
        "4. Finally, we’ll write a README file and update it to our GitHub repository."
      ],
      "metadata": {
        "id": "Xi57k6_X_oKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "この授業では、**人文学系のデータ**を用いて **RAGシステム** を構築します。\n",
        "\n",
        "内容：\n",
        "\n",
        "1. まず、RAGの基本を学びます。  \n",
        "2. 次に、人文学分野に関連するリソースを探ります。  \n",
        "3. その後、各自が作りたいRAGシステムのタイプについて議論します。  \n",
        "4. 最後に、READMEファイルを作成し、GitHubリポジトリに更新します。  "
      ],
      "metadata": {
        "id": "GZb_m4nQnwRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "REZYEFKj_VF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin by introducing what Retrieval-Augmented Generation (RAG) is:\n",
        "- **Retrieval-Augmented Generation (RAG)** is a technique that combines information retrieval with generative models. When generating responses, it first retrieves relevant content from an **external knowledge base** (such as documents or databases) and then uses that retrieved information to generate a response. This approach enables generative models to provide **more accurate and contextually relevant answers** by leveraging a larger information source.\n",
        "\n",
        "What we need to prepare:\n",
        "\n",
        "  1. Prepare for **Data**\n",
        "      - **Knowledge Base Construction (a vector database)**: A knowledge base is needed for retrieval. This can be a collection of documents, a database, or other structured information sources. The documents can be vectorized using embeddings (such as BERT or Sentence Transformers) and stored in a vector database (like FAISS or Milvus).\n",
        "      - **Retriever**: An efficient retriever is required to extract the most relevant content from the knowledge base. You can use Approximate Nearest Neighbor (ANN) search algorithms, such as FAISS, to implement the retrieval process.\n",
        "\n",
        "  2. **Model** selection\n",
        "       - **Retrieval Model**: You can use a pre-trained retrieval model (e.g., Dense Passage Retrieval, DPR) or a BERT-based model for information retrieval.\n",
        "       - **Generative Model**: A pre-trained generative model (like GPT-3 or T5) can be used to generate the text. In practice, the retrieval model is used to retrieve candidate documents or passages, which are then input into the generative model to produce the final answer.\n",
        "\n",
        "  3. **Implementation** workflow\n",
        "      \n",
        "      - **User Input Query**: The user inputs a query (a question or command).\n",
        "      - **Retriever Fetches Relevant Documents**: The retriever fetches the top K relevant documents from the knowledge base.\n",
        "      - **Generative Model Produces the Response**: The retrieved documents are combined with the user query and input into the generative model to generate the final response."
      ],
      "metadata": {
        "id": "yHEevQV7-rRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Retrieval-Augmented Generation (RAG)** とは、情報検索と生成モデルを組み合わせた手法です。  \n",
        "応答を生成する際、まず **外部知識ベース**（文書やデータベースなど）から関連情報を検索し、その取得情報をもとに応答を生成します。  \n",
        "この方法により、生成モデルはより多くの情報源を活用し、**正確で文脈に即した回答**を出力できるようになります。\n",
        "\n",
        "準備すべきもの\n",
        "\n",
        "1. **データの準備**\n",
        "\n",
        "- **知識ベースの構築（ベクトルデータベース）**  \n",
        "  検索のために知識ベースが必要です。これは文書群、データベース、または他の構造化情報源でも構いません。  \n",
        "  文書は BERT や Sentence Transformers などの埋め込みモデルを用いてベクトル化し、FAISS や Milvus などのベクトルデータベースに保存します。\n",
        "\n",
        "- **リトリーバ（Retriever）**  \n",
        "  知識ベースから最も関連性の高い内容を抽出するための効率的な検索器が必要です。  \n",
        "  近似最近傍探索（Approximate Nearest Neighbor, ANN）アルゴリズム（例：FAISS）を用いることで検索処理を実装できます。\n",
        "\n",
        "2. **モデルの選定**\n",
        "\n",
        "- **検索モデル（Retrieval Model）**  \n",
        "  事前学習済みの検索モデル（例：Dense Passage Retrieval, DPR）や BERT 系モデルを利用できます。\n",
        "\n",
        "- **生成モデル（Generative Model）**  \n",
        "  GPT-3 や T5 などの事前学習済み生成モデルを用いてテキストを生成します。  \n",
        "  実際には、検索モデルで候補文書や段落を取得し、それを生成モデルに入力して最終的な回答を得ます。\n",
        "\n",
        "3. **実装の流れ**\n",
        "\n",
        "- **ユーザの入力（クエリ）**  \n",
        "   ユーザが質問や指示を入力します。  \n",
        "- **リトリーバが関連文書を取得**  \n",
        "   リトリーバが知識ベースから関連性の高い上位 K 件の文書を取得します。  \n",
        "- **生成モデルが応答を生成**  \n",
        "   取得された文書とユーザのクエリを組み合わせ、生成モデルに入力して最終的な応答を生成します。\n"
      ],
      "metadata": {
        "id": "zBGfahCuou2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this Example Demo"
      ],
      "metadata": {
        "id": "EJ9HmUslDNKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s the data and tools we’ll be using for our RAG demo:\n",
        "\n",
        "1. **Data**\n",
        "   - **Source**: [Semiotics for Beginners](http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/), which is a great textbook for [Semiotics](https://en.wikipedia.org/wiki/Semiotics) (記号論), written by [Daniel Chandler](https://en.wikipedia.org/wiki/Daniel_Chandler). We’ll use the HTML version as our source document.\n",
        "   - **Embedding method**: We’ll use [OpenAIEmbeddings](https://platform.openai.com/docs/guides/embeddings) with the default model to convert our documents into vectors.\n",
        "   - **Vector DB method**: For storing these vectors, we’ll go with [Chroma](https://www.trychroma.com/), a lightweight, open-source embedding database.\n",
        "   - **Retrieval method**: Chroma’s default retrieval method will work just fine for what we need.\n",
        "\n",
        "2. **Model**\n",
        "   - **Retriever model**: We’ll stick with the OpenAIEmbedding default model for retrieval.\n",
        "   - **Generative model**: Again, we’ll use the OpenAI's `gpt-4o-mini` model for generation.\n",
        "\n",
        "3. **Implementation**: We’ll be building all of this using [LangChain](https://ja.wikipedia.org/wiki/LangChain).\n",
        "    - LangChain is a powerful framework for building applications powered by language models (LLMs), such as Retrieval-Augmented Generation (RAG) systems. It enables developers to combine models with external tools and databases in a modular way.\n"
      ],
      "metadata": {
        "id": "XvZy09pwEElr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "デモで使用するデータとツール\n",
        "\n",
        "1. **データ**\n",
        "\n",
        "- **ソース**  \n",
        "  [Semiotics for Beginners](http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/)  \n",
        "  記号論（[Semiotics](https://en.wikipedia.org/wiki/Semiotics)）の優れた教科書であり、  \n",
        "  [Daniel Chandler](https://en.wikipedia.org/wiki/Daniel_Chandler) によって執筆されています。  \n",
        "  このHTML版をソース文書として利用します。\n",
        "\n",
        "- **埋め込み方法（Embedding method）**  \n",
        "  [OpenAIEmbeddings](https://platform.openai.com/docs/guides/embeddings) のデフォルトモデルを使用し、文書をベクトルに変換します。\n",
        "\n",
        "- **ベクトルデータベース（Vector DB）**  \n",
        "  ベクトルの保存には軽量でオープンソースの埋め込みデータベース [Chroma](https://www.trychroma.com/) を使用します。\n",
        "\n",
        "- **検索方法（Retrieval method）**  \n",
        "  検索には Chroma のデフォルトリトリーバを使用します。今回の目的には十分です。\n",
        "\n",
        "2. **モデル**\n",
        "\n",
        "- **リトリーバモデル（Retriever model）**  \n",
        "  検索には OpenAIEmbedding のデフォルトモデルを使用します。\n",
        "\n",
        "- **生成モデル（Generative model）**  \n",
        "  生成には OpenAI の `gpt-4o-mini` モデルを使用します。\n",
        "\n",
        "3. **実装**\n",
        "\n",
        "- [RAGシステム](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) の構築には [LangChain](https://ja.wikipedia.org/wiki/LangChain) を利用します。  \n",
        "LangChain は、大規模言語モデル（LLM）を活用したアプリケーションを構築するための強力なフレームワークであり、  \n",
        "モデルと外部ツール・データベースをモジュール的に組み合わせることを可能にします。"
      ],
      "metadata": {
        "id": "NCXPEY55pvH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find humanities resources that you are interested in\n",
        "\n",
        "Some examples:\n",
        "\n",
        "Philosophy:\n",
        "\n",
        "- [Stanford Encyclopedia of Philosophy Archive](https://plato.stanford.edu/archIves/fall2024/index.html)\n",
        "\n",
        "Japanese literature, philology and linguistics:\n",
        "\n",
        "- [日文研公開データベース](https://www.nichibun.ac.jp/ja/db/)\n",
        "- [青空文庫](https://www.aozora.gr.jp/)\n",
        "- [雑誌『国語学』全文データベース](https://bibdb.ninjal.ac.jp/SJL/list.html)\n",
        "\n",
        "Linguistics:\n",
        "\n",
        "- [Database of Cross-Linguistic Colexifications](https://clics.clld.org/download)\n",
        "- [The World Atlas of Language Structures (WALS)](https://wals.info/)\n",
        "\n"
      ],
      "metadata": {
        "id": "hCcjV9bW_Xpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "人文学に関連するリソースを探してみましょう。以下はいくつかの例です。\n",
        "\n",
        "**哲学:**\n",
        "\n",
        "- [Stanford Encyclopedia of Philosophy Archive](https://plato.stanford.edu/archIves/fall2024/index.html)\n",
        "\n",
        "**日本文学・国語学・言語学:**\n",
        "\n",
        "- [日文研公開データベース](https://www.nichibun.ac.jp/ja/db/)\n",
        "- [青空文庫](https://www.aozora.gr.jp/)\n",
        "- [雑誌『国語学』全文データベース](https://bibdb.ninjal.ac.jp/SJL/list.html)\n",
        "\n",
        "**言語学:**\n",
        "\n",
        "- [Database of Cross-Linguistic Colexifications](https://clics.clld.org/download)\n",
        "- [The World Atlas of Language Structures (WALS)](https://wals.info/)\n"
      ],
      "metadata": {
        "id": "FpTFjEJQqhwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 | Required packages and settings"
      ],
      "metadata": {
        "id": "hQHk3SkNn6d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "RAGシステムを構築するために、以下のツールをインストールします。\n",
        "\n",
        "1. **`langchain`**  \n",
        "   言語モデルを用いたアプリケーションを構築するための主要パッケージです。  \n",
        "   データ処理、情報検索、応答生成の流れを整理して構築することができます。\n",
        "\n",
        "2. **`langchain-community`**  \n",
        "   LangChain コミュニティによる拡張ツールや追加機能を含むパッケージです。  \n",
        "   これらを利用することで、LangChain の機能をさらに強化できます。\n",
        "\n",
        "3. **`langchain_text_splitters`**  \n",
        "   大きな文書を小さなチャンクに分割するためのツールです。  \n",
        "   RAGシステムでは、長文を処理しやすくするために分割して利用します。\n",
        "\n",
        "4. **`langchain_chroma`**  \n",
        "   LangChain と **Chroma**（ベクトルデータベース）を接続するためのパッケージです。  \n",
        "   質問に答える際、最も関連性の高い情報を検索・取得する中核的な役割を担います。\n",
        "\n",
        "これらをインストールすることで、RAGシステムを段階的に構築するための準備が整います。\n",
        "\n",
        "インストールコマンド："
      ],
      "metadata": {
        "id": "mBZJUPTcqzle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the common tools we will need to install for our RAG system:\n",
        "\n",
        "1. **`langchain`**: This is the main package that contains everything we need to build applications using language models. It helps us organize how we process data, retrieve information, and generate responses.\n",
        "\n",
        "2. **`langchain-community`**: This package includes extra tools and features created by the LangChain community. These community contributions add new ways to use LangChain and can make our system even better.\n",
        "\n",
        "3. **`langchain_text_splitters`**: This package helps us break large documents into smaller chunks. In our RAG system, we’ll split long texts into pieces so that they are easier to process when generating responses.\n",
        "\n",
        "4. **`langchain_chroma`**: We’ll use this to connect LangChain with **Chroma**, a tool that stores and manages our data as vectors. This is an important part of retrieving the most relevant information when answering questions.\n",
        "\n",
        "By installing these, we’ll have all the necessary tools to build our system step by step!\n",
        "\n",
        "To install them, use the following command:"
      ],
      "metadata": {
        "id": "O9r3yhYCDcTM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krK19jSEZpLe",
        "outputId": "235ddb2e-e747-4f7d-b809-dcbbff886143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.6/602.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-community langchain_text_splitters langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional requirement to use LLM from OpenAI\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "dl-lBTP3vDtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6262a0a1-0485-4bdd-dbc1-bcb338f50c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional requirement to use open-source LLM from hugging face community\n",
        "!pip install -q torch transformers transformers accelerate sentence-transformers faiss-gpu"
      ],
      "metadata": {
        "id": "f0tcTDKnh97w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0acbd645-d96a-426f-cfaf-8c3e2606e878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain\n",
        "\n",
        "---\n",
        "\n",
        "Google Colab 上で実行する場合、LangChain を正しくインストールするために UTF-8 ロケールを有効にしておく必要があります。  "
      ],
      "metadata": {
        "id": "deJMQlhWDjjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "tBj5oo1BhtUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 | Large Language Model (LLM) selection"
      ],
      "metadata": {
        "id": "avRPMFnPt4vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "このデモでは、[LangChain](https://ja.wikipedia.org/wiki/LangChain) を用いて **大規模言語モデル（Large Language Model, LLM）** を利用します。  \n",
        "今回のデモでは [OpenAI](https://openai.com/) のモデルを使用しますが、\n",
        "以下のような他の選択肢も利用可能です。\n",
        "\n",
        "- Anthropic  \n",
        "- Azure  \n",
        "- Google  \n",
        "- Cohere  \n",
        "- NVIDIA  \n",
        "- FireworksAI  \n",
        "- Groq  \n",
        "\n",
        "これらのモデルを使用する場合、それぞれのサービスで **APIキー** を取得する必要があります。つまり、各プロバイダにアカウント登録を行い、アクセス用のAPIキーを発行する必要があります。  \n",
        "\n",
        "**なお、これらのサービスは無料ではありません。**\n",
        "\n",
        "異なるモデルのセットアップ手順については、公式の[LangChainドキュメント](https://python.langchain.com/docs/tutorials/rag/)に詳しいスクリプトと説明が掲載されています。\n",
        "\n",
        "有料サービスに加えて、無料で利用できるオープンソースのLLMもあります。たとえば、**Hugging Face Zephyr** を試すことができます。詳細は以下の [cookbook](https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_zephyr_langchain.ipynb) を参照してください。\n",
        "\n",
        "さらに幅広いオープンソースLLMの一覧は、[Open-source LLM leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)で確認できます。\n"
      ],
      "metadata": {
        "id": "Nz2k_XeAre8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we are using a **Large Language Model (LLM)** with [LangChain](https://ja.wikipedia.org/wiki/LangChain). For this demo, we will be utilizing the LLM from [OpenAI](https://openai.com/), but there are several other alternatives you can explore, including:\n",
        "\n",
        "- Anthropic\n",
        "- Azure\n",
        "- Google\n",
        "- Cohere\n",
        "- NVIDIA\n",
        "- FireworksAI\n",
        "- Groq\n",
        "\n",
        "If you choose to use one of these models, you will need to obtain the corresponding API keys. This means you’ll need to sign up for an account with the provider and generate API keys for access. **Please note that these services are not free**.\n",
        "\n",
        "The procedures for setting up different models can be found in the official [LangChain documentation](https://python.langchain.com/docs/tutorials/rag/), which provides setup scripts for various models.\n",
        "\n",
        "In addition to paid services, there are also free, open-source LLMs you can use. For instance, you could try **Hugging Face Zephyr**. You can find more information in this helpful [cookbook](https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_zephyr_langchain.ipynb).\n",
        "\n",
        "For a broader range of open-source LLM options, take a look at the [Open-source LLM leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard).\n"
      ],
      "metadata": {
        "id": "61q94Cp2vCGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 | Non-free LLM, e.g., OpenAI gpt-4o"
      ],
      "metadata": {
        "id": "hJDbkHWKhKm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "有料の LLM（たとえば **OpenAI の GPT-4o**）を使用して、埋め込み・検索・テキスト生成などのタスクを実行します。以下の手順で、APIキーを安全に入力し、モデルを初期化します。"
      ],
      "metadata": {
        "id": "3BULGjdv0vQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set up and use a non-free LLM, like **OpenAI's GPT-4o**, for tasks such as embedding, retrieval, and text generation. We will securely input our API key and initialize the model."
      ],
      "metadata": {
        "id": "lVFCGk4eFTDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-free LLM, e.g., OpenAI gpt-4o\n",
        "# you need to input the api key\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # for embedding, retriever and generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJKNdj5Hpt29",
        "outputId": "b635e736-25ec-4fdf-c7ce-320019e6b369"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 | Alternative: Free LLM, e.g., Hugging Face Zephyr"
      ],
      "metadata": {
        "id": "NNEhQcLohduX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "無料の代替案をご希望の場合は、**Hugging Face Zephyr**のようなモデルを使用できます。これはHugging Faceを通じて利用可能なオープンソースのLLMで、多くのプロジェクトにとってコスト効率の良い選択肢となります。\n",
        "\n",
        "ただし、**ご注意ください**：RAMの制限により、このオプションは**Google Colab**での実行には適していない可能性があります。*斜体テキスト*"
      ],
      "metadata": {
        "id": "bScJodYvsmN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd prefer a free alternative, you can use models like **Hugging Face Zephyr**. It is an open-source LLM available through Hugging Face, making it a cost-effective choice for many projects.\n",
        "\n",
        "However, **please note** that due to RAM limitations, this option may not be suitable for running on **Google Colab**. *italicized text*\n"
      ],
      "metadata": {
        "id": "ZnsgCdFyFkqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN in Colab\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "zfnnya3tDD3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN in Colab\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "4Z8KOzNYoy9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 | (Humanities Domain-specific) Documents / Data source selection"
      ],
      "metadata": {
        "id": "kjPfzdb1nGDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "このプロジェクトでは、**人文学**に関連するドメイン固有の文書を使用してRAGシステムを構築します。例として、記号論の入門教科書である[Semiotics for Beginners](http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/)を使用しました。\n",
        "\n",
        "この特定のソースはHTML文書ですが、LangChainは以下を含む様々な文書タイプをサポートしています：\n",
        "\n",
        "LangChainはHTML以外にも様々な文書タイプをサポートしています：\n",
        "\n",
        "- **PDF**：PDFLoaderを使用してPDFファイルからテキストを抽出します。\n",
        "- **TXT**：プレーンテキストファイルを直接処理します。\n",
        "- **HTML**：HTML文書やウェブページを解析します。\n",
        "- **DOC/DOCX**：`python-docx`のようなPythonライブラリを使用してMicrosoft Wordファイルを扱います。\n",
        "- **EPUB**：EPUB解析ライブラリを使用して電子書籍を扱います。\n",
        "- **CSV**：CSVファイルを使用してテーブルからデータを抽出します。\n",
        "\n",
        "プロジェクトのニーズに基づいて、これらのフォーマットを自由に探索してください。詳しいガイダンスについては、LangChainのドキュメントを参照するか、関連するPythonライブラリを検索してください。\n",
        "\n",
        "これらのフォーマットやニーズに合った他のフォーマットを自由に使用してください。**ドキュメントローダー**については https://python.langchain.com/docs/integrations/document_loaders/ を参照してください。\n",
        "\n",
        "**重要な注意事項**：\n",
        "外部リソースを扱う際は、選択した文書のライセンス条項を必ず確認してください。適切な許可を得ずに、著作権で保護されたリソースをリポジトリに直接含めることはできません。使用を予定している外部リソースについては、必ずライセンスや許可を確認してください。"
      ],
      "metadata": {
        "id": "x1nOLLoZs6da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will be using domain-specific documents related to the **Humanities** to build our RAG system. As an example, I’ve used [Semiotics for Beginners](http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/), which is an introductory textbook on semiotics (記号論).\n",
        "\n",
        "This particular source is an HTML document, but LangChain supports various document types, including:\n",
        "\n",
        "LangChain supports various document types beyond HTML, including:\n",
        "\n",
        "- **PDF**: Use PDFLoader to extract text from PDF files.\n",
        "- **TXT**: Process plain text files directly.\n",
        "- **HTML**: Parse HTML documents or web pages.\n",
        "- **DOC/DOCX**: Use Python libraries like `python-docx` to handle Microsoft Word files.\n",
        "- **EPUB**: Use EPUB parsing libraries to work with e-books.\n",
        "- **CSV**: Extract data from tables using CSV files.\n",
        "\n",
        "Feel free to explore these formats based on your project needs. You can refer to LangChain's documentation or search for relevant Python libraries for further guidance.\n",
        "\n",
        "Feel free to use any of these formats or others that match your needs. For **Document Loader** see https://python.langchain.com/docs/integrations/document_loaders/.\n",
        "\n",
        "**Important Note**:\n",
        "Since we are working with external resources, make sure you check the licensing terms of the documents you choose. We cannot include copyrighted resources directly in our repository without obtaining proper permission. Always confirm the license or permissions for any external resources you plan to use.\n"
      ],
      "metadata": {
        "id": "6gOaAUMzGbc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2.1 | Source: [Semiotics for Beginners](http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/)\n",
        "\n"
      ],
      "metadata": {
        "id": "7lO73R_PiJ5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "まず、`requests`ライブラリを使用してウェブページのコンテンツを取得し、`bs4`の`BeautifulSoup`を使用してHTMLを解析します。次に、スクリプトはすべてのリンク（`href`属性を持つ`<a>`タグ）を抽出して処理し、アンカーや`mailto:`リンクなどの不要なリンクを除外し、相対URLを絶対URLに変換します。"
      ],
      "metadata": {
        "id": "55VLnKwVtI1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We firstly use the `requests` library to fetch the content of a webpage, and `BeautifulSoup` from `bs4` to parse the HTML. The script then extracts all the links (`<a>` tags with `href` attributes) and processes them, filtering out unwanted links like anchors or `mailto:` links, and converting relative URLs into absolute ones.\n"
      ],
      "metadata": {
        "id": "PeUq7Z75IqBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "# Fetch the webpage content\n",
        "url = \"http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = bs4.BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Extract all <a> tags with href attributes\n",
        "links = []\n",
        "for a_tag in soup.find_all('a', href=True):\n",
        "    href = a_tag['href']\n",
        "    # Filter out unwanted anchors or mailto links\n",
        "    if not href.startswith(\"#\") and not href.startswith(\"mailto:\"):\n",
        "        # If it's a relative link, convert it to an absolute link\n",
        "        if not href.startswith(\"http\"):\n",
        "            href = url + href\n",
        "        links.append(href)"
      ],
      "metadata": {
        "id": "T0LwZfOspWRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links"
      ],
      "metadata": {
        "id": "8sI4K2uRtghs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "次に、上記のリンクから関連するテキストコンテンツを読み込みます。特に段落レベルの情報に焦点を当てます。これにより、取得したコンテンツがより構造化され、検索拡張生成（RAG）プロセスの後の段階で使いやすくなります。"
      ],
      "metadata": {
        "id": "q06bsBR1tXDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we load relevant textual content from the above links, specifically focusing on paragraph-level information. This makes the retrieved content more structured and usable for later stages in the Retrieval-Augmented Generation (RAG) process.\n"
      ],
      "metadata": {
        "id": "B_Y1bBOgJT_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=tuple(links),  # Provide the list of URLs as a tuple\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\"p\")  # Parse only <p> tags for content extraction\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzGPCXaMl8D3",
        "outputId": "7456ad4a-e794-4755-974e-27fa2ab2752b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "qq68E3ZttkJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 | Data preprocess"
      ],
      "metadata": {
        "id": "7lGF4pActNSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "ここでは、LangChainの`RecursiveCharacterTextSplitter`を使用して、大きな文書をより小さく管理しやすいチャンクに分割します。これは、小さな部分に分けて処理またはインデックス化する必要がある長いテキストを扱う際に特に便利です。"
      ],
      "metadata": {
        "id": "lnEIfX7NttdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use the `RecursiveCharacterTextSplitter` from LangChain to split large documents into smaller, manageable chunks. This is especially useful when working with long texts that need to be processed or indexed in smaller parts.\n"
      ],
      "metadata": {
        "id": "ST7irtvwK3O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "VjxYiGROqkEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 | Vector Database (the knowledge base)"
      ],
      "metadata": {
        "id": "GGH-9zEfuuQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "このセクションでは、非無料の埋め込みモデル（OpenAI）と、埋め込みを保存するための軽量なオープンソースのベクターデータベースである**Chroma**を使用してベクターデータベースを構築します。"
      ],
      "metadata": {
        "id": "TCT0yN73t6EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we are building a vector database using OpenAI's non-free embedding model and **Chroma**, which is a lightweight, open-source vector database for storing embeddings.\n"
      ],
      "metadata": {
        "id": "h0E3JrQoMPwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 | Embedding model"
      ],
      "metadata": {
        "id": "zQwoh9M4uEt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "埋め込みモデル（Embedding Model）は、テキストデータを数値ベクトルに変換するための機械学習モデルです。\n",
        "\n",
        "このモデルは、単語や文章などのテキストを高次元の数値ベクトル（通常は数百から数千次元）に変換します。これにより、コンピュータがテキストの意味を理解し、処理できるようになります。\n",
        "\n",
        "**主な特徴：**\n",
        "- **意味の保持**：似た意味を持つテキストは、ベクトル空間上で近い位置に配置されます\n",
        "- **数学的演算が可能**：テキスト間の類似度計算や検索が高速に実行できます\n",
        "- **RAGシステムでの役割**：ユーザーの質問と文書の内容を同じベクトル空間に変換し、関連性の高い情報を効率的に検索できるようにします\n",
        "\n",
        "例えば、「犬」と「子犬」は意味的に近いため、ベクトル空間上でも近い位置に配置され、「車」のような無関係な単語からは離れた位置に配置されます。"
      ],
      "metadata": {
        "id": "UZbSyzo-vi7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An embedding model is a machine learning model that converts text data into numerical vectors.\n",
        "\n",
        "This model transforms text such as words or sentences into high-dimensional numerical vectors (typically hundreds to thousands of dimensions). This enables computers to understand and process the meaning of text.\n",
        "\n",
        "**Key Features:**\n",
        "- **Semantic Preservation**: Texts with similar meanings are positioned close to each other in the vector space\n",
        "- **Mathematical Operations**: Enables fast computation of similarity between texts and efficient search operations\n",
        "- **Role in RAG Systems**: Converts both user queries and document content into the same vector space, allowing for efficient retrieval of highly relevant information\n",
        "\n",
        "For example, \"dog\" and \"puppy\" are semantically similar, so they are positioned close to each other in the vector space, while unrelated words like \"car\" are positioned farther away."
      ],
      "metadata": {
        "id": "AQKKaOU7vlVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "9EdFqEhB86np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 | Database"
      ],
      "metadata": {
        "id": "PDR4VCyMrgrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 | Creating a New Database"
      ],
      "metadata": {
        "id": "fTf6g0isurbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "新しいベクターデータベース（DB）をゼロから作成する必要がある場合、このコードは非無料の埋め込みモデルによって作成された埋め込みを使用してChromaでデータベースを初期化し、保存します。また、このデータベースにアクセスするためのリトリーバーを設定することもできます。"
      ],
      "metadata": {
        "id": "2ks7z1Qgv771"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to create a new vector database (DB) from scratch, this code will initialize the database using **Chroma** and store the embeddings created by the non-free embedding model. You can also set up a retriever to access this database.\n",
        "\n"
      ],
      "metadata": {
        "id": "xYSfZi2KqwR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=embedding_function,\n",
        "  persist_directory=\"./chroma_db\",\n",
        ")"
      ],
      "metadata": {
        "id": "fUW81l2yumUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 | Alternative: Reusing an Existing Database"
      ],
      "metadata": {
        "id": "67TLRdePrVd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "ベクターデータベースをゼロから再構築する代わりに、事前に準備された既存の**Chroma**データベースを使用できます。これにより、以前に作成されたデータベースから埋め込みを直接読み込むことで、時間とリソースを節約できます。"
      ],
      "metadata": {
        "id": "Et5dc1Gawe7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of rebuilding the vector database from scratch, we can use an existing **Chroma** database that has been prepared beforehand. This allows us to save time and resources by directly loading the embeddings from a previously created database.\n"
      ],
      "metadata": {
        "id": "modYRd7npzhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I download my existing Chroma DB from my google drive\n",
        "!gdown --folder https://drive.google.com/drive/folders/1Y1QT3zWIc0oPGJbGdSxP_MWvhz1CAecL?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3BtPMgnilr",
        "outputId": "4e151f4c-3479-4ea9-e2cd-85608ce96b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1-1GjnKu2NXTrfgPBXGJA1WAl3KFKMH0w 04226cce-6d14-486a-a1b1-b65ba036747b\n",
            "Processing file 1-EPv0jHtH6Qe-NkqQk8iMgfIXJfH99vM data_level0.bin\n",
            "Processing file 1-DMSdwMRCpOE5-mFRspjQg0oc4Cz9exX header.bin\n",
            "Processing file 1-PldyhMMlV2ZEuzENso42JYG2esl1ahz length.bin\n",
            "Processing file 1-Ce6LKotMq6f1iI_umj6QeVltwkfiPB1 link_lists.bin\n",
            "Processing file 1-5A0qnc5kUR4oxiicWcBayv-xDhCAVSu chroma.sqlite3\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-EPv0jHtH6Qe-NkqQk8iMgfIXJfH99vM\n",
            "To: /content/chroma_db/04226cce-6d14-486a-a1b1-b65ba036747b/data_level0.bin\n",
            "100% 12.4M/12.4M [00:00<00:00, 53.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-DMSdwMRCpOE5-mFRspjQg0oc4Cz9exX\n",
            "To: /content/chroma_db/04226cce-6d14-486a-a1b1-b65ba036747b/header.bin\n",
            "100% 100/100 [00:00<00:00, 532kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-PldyhMMlV2ZEuzENso42JYG2esl1ahz\n",
            "To: /content/chroma_db/04226cce-6d14-486a-a1b1-b65ba036747b/length.bin\n",
            "100% 4.00k/4.00k [00:00<00:00, 21.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-Ce6LKotMq6f1iI_umj6QeVltwkfiPB1\n",
            "To: /content/chroma_db/04226cce-6d14-486a-a1b1-b65ba036747b/link_lists.bin\n",
            "0.00B [00:00, ?B/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5A0qnc5kUR4oxiicWcBayv-xDhCAVSu\n",
            "To: /content/chroma_db/chroma.sqlite3\n",
            "100% 20.3M/20.3M [00:01<00:00, 17.4MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vector DB from existing DB\n",
        "vectorstore = Chroma(\n",
        "    embedding_function=embedding_function,\n",
        "    persist_directory=\"./chroma_db\"  # vector DB path\n",
        ")"
      ],
      "metadata": {
        "id": "qbIw96jSqA44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 | Retriever"
      ],
      "metadata": {
        "id": "YoOCeicMgmaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "LangChainでは、このデータベースにアクセスするためのリトリーバーを1行で設定できます。"
      ],
      "metadata": {
        "id": "-9hZjwhiwny8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can set up a retriever to access this database in LangChain with one line."
      ],
      "metadata": {
        "id": "1PJ1aG-9huWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever using vector DB\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "BY3wiQ5XgsA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 | Alternative: Buiding DB using free embedding model and FAISS"
      ],
      "metadata": {
        "id": "ch7FVwdJJWbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAIのような非無料のLLMやChromaの代わりに、**無料のLLM**と**FAISS**を使用してベクターデータベースを構築する代替アプローチもあります。\n",
        "\n",
        "説明：\n",
        "\n",
        "1. **無料のモデル**：\n",
        "   - OpenAIのような有料サービスを使用する代わりに、Hugging Faceから入手可能な無料のモデルを使用して文書の埋め込みを生成できます。これらの無料モデルは、実験と開発のコストを削減できます。\n",
        "\n",
        "2. **FAISS**：\n",
        "   - **FAISS**（Facebook AI Similarity Search）は、密ベクトルの効率的な類似性検索とクラスタリングのために設計されたオープンソースライブラリです。ベクトル埋め込みの保存と取得において、Chromaの優れた代替手段となります。\n",
        "   - FAISSは、大規模なデータセットを扱う際の速度とスケーラビリティで特に知られています。ただし、FAISSは主にメモリ上で動作するため、大量のリソースを必要とする可能性があり、RAMが制限されているGoogle Colabのようなプラットフォームでは制約となる場合があります。\n",
        "\n",
        "3. **なぜFAISSを使用するのか？**：\n",
        "   - FAISSは、ベクトル類似性検索のための高性能で無料のソリューションを提供します。有料APIの使用が選択肢にないプロジェクトの場合、FAISSは検索拡張生成（RAG）システムを構築するための実行可能な代替手段となります。\n",
        "   - 大規模なデータセットを処理でき、ベクトル埋め込みの検索に最適化されているため、学術分野や企業アプリケーションの両方で人気の選択肢となっています。\n",
        "\n",
        "**重要な注意事項**：\n",
        "Google ColabのRAM制限により、このアプローチはColab環境では期待通りに動作しない可能性があります。"
      ],
      "metadata": {
        "id": "d45ugsMYw_Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have an alternative approach to building a vector database using a **free embedding models** and **FAISS** instead of a non-free LLM like OpenAI and Chroma.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. **Free embedding model**:\n",
        "   - Instead of using a paid service like OpenAI, free models such as those available from Hugging Face can be used to generate embeddings for the documents. These free models can reduce the cost of experimentation and development.\n",
        "\n",
        "2. **FAISS**:\n",
        "   - **FAISS** (Facebook AI Similarity Search) is an open-source library designed for efficient similarity search and clustering of dense vectors. It is an excellent alternative to Chroma for storing and retrieving vector embeddings.\n",
        "   - FAISS is particularly known for its speed and scalability when working with large datasets. However, because FAISS operates primarily in memory, it may require significant resources, which can be a limitation on platforms like Google Colab due to restricted RAM.\n",
        "\n",
        "3. **Why Use FAISS?**:\n",
        "   - FAISS offers a high-performance, free solution for vector similarity search. For projects where using paid APIs is not an option, FAISS provides a viable alternative for building a retrieval-augmented generation (RAG) system.\n",
        "   - It can handle large datasets and is well-optimized for searching through vector embeddings, making it a popular choice for academic and enterprise applications alike.\n",
        "\n",
        "**Important Note**:\n",
        "Due to RAM limitations in Google Colab, this approach may not work as expected in the Colab environment.\n"
      ],
      "metadata": {
        "id": "6WhgHQS-N7Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN in Colab\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "db = FAISS.from_documents(\n",
        "    splits,\n",
        "    HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
        ")"
      ],
      "metadata": {
        "id": "rx8gGwBGJcjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 | Prompt (question + context) template"
      ],
      "metadata": {
        "id": "hyBtDXZYmjTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "*プロンプトとは何か？*\n",
        "\n",
        "**プロンプト**とは、LLMに与えられる最初の入力または質問であり、モデルが応答すべきタスクやクエリを定義するものです。RAGシステムにおいて、プロンプトは通常以下を含みます：\n",
        "1. **ユーザーのクエリ**：ユーザーから提供される主要な質問またはタスク\n",
        "2. **コンテキストとして取得された文書**：ベクターデータベースからリトリーバーによって取得された追加の情報。これらはLLMがクエリのコンテキストを理解し、より正確な応答を生成するのに役立ちます。\n",
        "\n",
        "*RAGにおけるプロンプトの動作*：\n",
        "\n",
        "1. **ユーザークエリ**：\n",
        "   - ユーザーが質問をするか、入力を提供します（メインプロンプト）。\n",
        "   \n",
        "2. **検索**：\n",
        "   - リトリーバーがベクターデータベースで関連文書を検索し、クエリに最も類似した文書のセットを返します。\n",
        "   \n",
        "3. **拡張プロンプト**：\n",
        "   - 取得された文書は元のユーザークエリと組み合わされて、**拡張プロンプト**を作成します。この拡張プロンプトは、ユーザーの入力と関連するコンテキストの両方をLLMに提供し、詳細で正確な応答を生成しやすくします。\n",
        "   \n",
        "4. **応答生成**：\n",
        "   - LLMは拡張プロンプトを処理し、ユーザーのクエリと取得された文書からの追加コンテキストの両方に基づいた応答を生成します。"
      ],
      "metadata": {
        "id": "n6Gh0YnmxQqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What is a Prompt*?\n",
        "\n",
        "A **prompt** is the initial input or question given to the LLM, which defines the task or query for the model to respond to. In a RAG system, the prompt often includes:\n",
        "1. **The User’s Query**: This is the main question or task provided by the user.\n",
        "2. **Retrieved Documents as Context**: These are additional pieces of information retrieved by the retriever from the vector database. They help the LLM understand the context of the query and generate more accurate responses.\n",
        "\n",
        "*How Prompts Work in RAG*:\n",
        "\n",
        "1. **User Query**:\n",
        "   - The user asks a question or provides an input (the main prompt).\n",
        "   \n",
        "2. **Retrieval**:\n",
        "   - The retriever searches the vector database for relevant documents and returns a set of documents that are most similar to the query.\n",
        "   \n",
        "3. **Augmented Prompt**:\n",
        "   - The retrieved documents are then combined with the original user query to create an **augmented prompt**. This augmented prompt provides the LLM with both the user’s input and relevant context, making it easier to generate a detailed and accurate response.\n",
        "   \n",
        "4. **Response Generation**:\n",
        "   - The LLM processes the augmented prompt and generates a response that is informed by both the user’s query and the additional context from the retrieved documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "NywTkTRUPxPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate class is used to create a custom prompt template for the LLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Prompt template (`<||>` denotes section; `{}` denotes placeholder):\n",
        "# `<|system|>`: defines the system's role, provide general instructions to LLM\n",
        "# `{context}`: a placeholder for the context retrieved from the knowledge base.\n",
        "#   It's dynamically replaced with relevant information related to the user's query.\n",
        "# `<|user|>`: represents the user's input, where the user asks a question.\n",
        "# {question}: a placeholder which will be replaced with the user's actual question.\n",
        "# `<|assistant|>`: This is where the language model (assistant) generates its response,\n",
        "#   based on both the context and the question.\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "keep the answer as concise as possible.\n",
        "Use markdown formatting when displaying code.\n",
        "Emphasis should be used to terminologies.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#  Create the PromptTemplate Instance\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\n",
        "        \"context\",\n",
        "        \"question\",\n",
        "        \"sources\"\n",
        "        ],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Or using availible prompts\n",
        "# from langchain import hub\n",
        "# from langchain_chroma import Chroma\n",
        "\n",
        "# prompt use\n",
        "# prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "VNXQXPOQm2iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 | Optional: Output formatter"
      ],
      "metadata": {
        "id": "yIlU7-gdjPk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 | Document (context) formatter"
      ],
      "metadata": {
        "id": "KKTyeyeezLL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "処理された文書を単一の文字列にフォーマットする関数を定義します。これにより、表示やさらなる処理が簡単になります。複数の文書チャンクの内容を出力または可視化したい場合に便利です。"
      ],
      "metadata": {
        "id": "pKVcKLIOxcrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to format the processed documents into a single string for easier display or further processing. This is helpful when you want to output or visualize the content of multiple document chunks.\n"
      ],
      "metadata": {
        "id": "qAmU9p4WLc0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "JXNrS8lKWyNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 | Source information formatter"
      ],
      "metadata": {
        "id": "rzBXi_U5zPy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "次に、取得された文書からソース（メタデータ）を抽出してフォーマットする関数を定義します。これは、ソース情報（URLや文書タイトルなど）をコンテキストの一部として言語モデル（LLM）に渡したい場合に特に便利です。"
      ],
      "metadata": {
        "id": "t1C6r9ufxlPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a function to extract and format the sources (metadata) from the retrieved documents. This is particularly useful when you want to pass the source information (such as URLs or document titles) as part of the context to a language model (LLM).\n"
      ],
      "metadata": {
        "id": "7hxGbIBlLsxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom function to format sources from retrieved documents\n",
        "def format_sources(docs):\n",
        "    \"\"\"\n",
        "    Extracts and formats the sources (metadata) from the retrieved documents.\n",
        "    This function ensures that the source information is passed to the LLM as part of the context.\n",
        "    \"\"\"\n",
        "    # Extract sources (e.g., URLs or document titles) from metadata\n",
        "    sources = [doc.metadata.get('source', 'Unknown Source') for doc in docs]\n",
        "    # Join the sources into a single string to pass to the LLM\n",
        "    return \"Sources:\\n\" + \"\\n\".join(sources)"
      ],
      "metadata": {
        "id": "hEgZybHj7jSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 | Answer formatter"
      ],
      "metadata": {
        "id": "XatXO3oyzo_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "次のコードは、Jupyterノートブックや、IPythonの表示機能をサポートする環境内で、出力（回答）をきれいにフォーマットされたMarkdown形式で表示するために使用されます。"
      ],
      "metadata": {
        "id": "snyK66rvzFdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to display the output (answer) in a neatly formatted **Markdown** format within a Jupyter notebook or any environment that supports IPython display functions.\n"
      ],
      "metadata": {
        "id": "SkfMp9EARU6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To  display the output (answer) in a neatly formatted Markdown format\n",
        "# within a Jupyter notebook\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def display_answer(answer):\n",
        "    display(Markdown(answer))"
      ],
      "metadata": {
        "id": "grJu0j_WvGWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 | Overall workflow by combining the above components"
      ],
      "metadata": {
        "id": "CCN9YfMx_QYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "+----------+     +-------------+     +-------------+     +---------+\n",
        "| Question |---->| Retrieval   |---->|   Context   |---->| Answer  |\n",
        "|          |     |  (search)   |     |(Docs merged)|     |(LLM Gen)|\n",
        "+----------+     +-------------+     +-------------+     +---------+\n",
        "                       ^\n",
        "                       |\n",
        "                 +----------------+\n",
        "                 |    Database /  |\n",
        "                 |   (documents)  |\n",
        "                 +----------------+\n",
        "```"
      ],
      "metadata": {
        "id": "ePbHBhrO4PXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "このセクションでは、上記のコンポーネントを組み合わせて、検索拡張生成（RAG）システムを使用した完全なワークフローを構築する方法について説明します。\n",
        "\n",
        "**RAGchain**は、LangChainフレームワーク内の専門的なコンポーネントで、検索と生成を1つのシームレスなワークフローで組み合わせるプロセスを効率化するよう設計されています。RAGchainがこのプロセスをより効率的にする方法は以下の通りです：\n",
        "- **リトリーバーとジェネレーターの統合**：RAGchainは、リトリーバー（関連文書を見つける）とジェネレーター（取得された文書に基づいて最終的な応答を作成する）の両方を1つの統合されたワークフローに組み合わせます。\n",
        "- **コンテキストを考慮した生成**：検索を生成プロセスに直接統合することで、RAGchainは生成される応答がより正確で文脈的に関連性の高いものになることを保証します。\n",
        "- **簡略化されたワークフロー**：RAGchainを使用すると、検索と生成の個別のステップを手動で処理する必要がありません。フレームワークがプロセス全体を処理するため、実装と保守がより簡単になります。"
      ],
      "metadata": {
        "id": "B7FXP2bAyaDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we discuss how to combine the above components to build a complete workflow using a Retrieval-Augmented Generation (RAG) system."
      ],
      "metadata": {
        "id": "OXu0Re-QO4W-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAGchain** is a specialized component within the LangChain framework designed to streamline the process of combining retrieval and generation in one seamless workflow. Here’s how RAGchain makes this process more efficient:\n",
        "- **Integrates Retriever and Generator**: RAGchain combines both the retriever (which finds relevant documents) and the generator (which creates the final response based on the retrieved documents) into one unified workflow.\n",
        "- **Context-aware Generation**: By integrating retrieval directly into the generation process, RAGchain ensures that the generated responses are more accurate and contextually relevant.\n",
        "- **Simplified Workflow**: With RAGchain, you don’t need to manually handle the separate steps of retrieval and generation; the framework takes care of the entire process, making it easier to implement and maintain.\n"
      ],
      "metadata": {
        "id": "uGDzBjAIRvTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an output parser used to convert the raw output from the LLM into a string.\n",
        "# It ensures the LLM’s output is formatted as readable text.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# This is a utility that simply passes the input data through without any modifications.\n",
        "# It’s used when you want to allow the data to flow through a pipeline step unchanged.\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# LLM chain:\n",
        "# The prompt is sent to the LLM.\n",
        "# The LLM generates a response.\n",
        "# The generated response is passed to StrOutputParser for formatting.\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# RAG chain:\n",
        "# 1. `context`: contextual information retrieved from the knowledge base.\n",
        "#     The value is a pipeline that first uses retriever to find relevant documents,\n",
        "#     and then passes the retrieved documents through format_docs to format them for further use.\n",
        "#     Additionally, the `format_sources` step is added to include the sources.\n",
        "# 1.1. `retriever`: Retrieves relevant documents from the knowledge base based on the user's query.\n",
        "# 1.2. `format_docs`: Formats the retrieved documents.\n",
        "# 1.3. `format_sources`: Extracts the sources of the retrieved documents (e.g., URLs).\n",
        "# 2. `question`: the user's input query, which is passed directly through RunnablePassthrough().\n",
        "#     The RunnablePassthrough simply forwards the original question without any modification.\n",
        "# 3. `llm_chain`: Takes these inputs (context + sources + question) and generates a response using the LLM.\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,    # Retrieval Step for context\n",
        "        \"sources\": retriever | format_sources, # Retrieve and format sources\n",
        "        \"question\": RunnablePassthrough()      # Prompt Generation\n",
        "    }\n",
        "    | llm_chain                                # Generation Step\n",
        ")"
      ],
      "metadata": {
        "id": "XkdRSsZGvunv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 | Showcase"
      ],
      "metadata": {
        "id": "sD2TqS6PtV2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "外部知識（教科書の第2章から）に関する以下の文脈について、RAGシステムに問い合わせを試みました：\n",
        "\n",
        "> エーコは、３種類の記号負荷体を列挙している。その区分は、部分的に材料の種類に関係することは注目される：\n",
        "> '同じタイプの、単数または複数のトークン（複製）がある記号'（例えば、印刷された単語、または同じ色の全く同じモデルの自動車）；\n",
        "> 'そのトークンがあるタイプから製造されたとしても、材質的には独自性を持った記号'（例えば、ある人が話したまたは手書きされた言葉）；\n",
        "> 'そのトークンがそのタイプである、またはタイプとトークンが同一の記号'（例えば、唯一の原画である油絵またはダイアナ王女の結婚衣裳)。 (Eco 1976, 178ff)\n",
        "\n",
        "以下で、ベクターデータベースなしでは、LLMの回答が望ましくないものであることが観察できます。対照的に、RAGチェーンによって生成された回答は、より正確で文脈的に適切です。\n",
        "\n",
        "私の質問は以下の通りです：\n",
        "\n",
        "> トークンとタイプに関連して、エーコは３種類の記号負荷体を提示している。その区分について教えていただけませんか。"
      ],
      "metadata": {
        "id": "P_Q_69RuyvHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I attempted to query the RAG system regarding the following context (the external knowledge), which is from the second chapter of the textbook:\n",
        "\n",
        "> エーコは、３種類の記号負荷体を列挙している。その区分は、部分的に材料の種類に関係することは注目される：\n",
        "> ‘同じタイプの、単数または複数のトークン（複製）がある記号’（例えば、印刷された単語、または同じ色の全く同じモデルの自動車）；\n",
        "> ‘そのトークンがあるタイプから製造されたとしても、材質的には独自性を持った記号’（例えば、ある人が話したまたは手書きされた言葉）；\n",
        "> ‘そのトークンがそのタイプである、またはタイプとトークンが同一の記号’（例えば、唯一の原画である油絵またはダイアナ王女の結婚衣裳)。 (Eco 1976, 178ff)\n",
        "\n",
        "Below, we can observe that without the vector database, the LLM's answers are less desirable. In contrast, answers generated by the RAG chain are more accurate and contextually appropriate.\n",
        "\n",
        "My question is\n",
        "\n",
        "> トークンとタイプに関連して、エーコは３種類の記号負荷体を提示している。その区分について教えていただけませんか。"
      ],
      "metadata": {
        "id": "b8hrFA98w0ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My question\n",
        "query = \"トークンとタイプに関連して、エーコは３種類の記号負荷体を提示している。その区分について教えていただけませんか。\""
      ],
      "metadata": {
        "id": "YuiQ2SFSvP0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 | Domain-specific answering (RAG)"
      ],
      "metadata": {
        "id": "_2TlpqrI-iX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer with the context from our vector DB\n",
        "anwser = rag_chain.invoke(query)\n",
        "display_answer(anwser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "WdHu__FXCSxs",
        "outputId": "b0e25198-bdec-4e55-8201-667840922c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "エーコは、**記号負荷体**を次の三種類に区分しています：\n\n1. **同じタイプの、単数または複数のトークン（複製）**：\n   - 例：印刷された単語や同じ色の全く同じモデルの自動車。\n\n2. **そのトークンがあるタイプから製造されたとしても、材質的には独自性を持った記号**：\n   - 例：ある人が話した言葉や手書きの文字。\n\n3. **そのトークンがそのタイプである、またはタイプとトークンが同一の記号**：\n   - 例：唯一の原画である油絵や特定の結婚衣装。\n\nこの区分は、記号の材料の種類に関連しています。 \n\nThanks for asking!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 | Domain-general answering"
      ],
      "metadata": {
        "id": "m-UxBoxd-pU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer without external knowledge\n",
        "answer_without_knowledge = llm_chain.invoke({\"context\":\"\", \"question\": query})\n",
        "display_answer(answer_without_knowledge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "oveu0K8Nuwx-",
        "outputId": "eeb645e7-e501-4ed7-b168-33ef5f552b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "エーコは、トークンとタイプに関連して以下の3種類の**記号負荷体**を提示しています：\n\n1. **インデックス**：実際の現象や対象を指し示す記号。例としては、煙が火を示す場合などがあります。\n2. **アイコン**：対象の特性を模倣または類似させた記号。例えば、絵画や写真などがこれにあたります。\n3. **シンボル**：恣意的に決定された関係を持つ記号で、文化や社会によって意味が変わることがあります。言語や数学記号が例として挙げられます。\n\nこれらの区分は、記号がどのように意味を持つかを理解するための重要な枠組みとなっています。  \nThanks for asking!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 | Extracting sources"
      ],
      "metadata": {
        "id": "IlnQhG3w-wsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "次のブロックは、関連文書を取得し、そのソースを抽出し、生成された回答とソースを別々に表示する方法を示しています。これは、RAGシステムを使用する際に透明性とコンテキストを提供するのに便利です。"
      ],
      "metadata": {
        "id": "TGn4BEYc0eGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block is to show how to retrieve relevant documents, extract their sources, and then display both the generated answer and the sources separately. This can be useful for providing transparency and context when using a RAG system.\n"
      ],
      "metadata": {
        "id": "YX8EUr0ZSSHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separately extract the sources for independent use\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "sources = format_sources(retrieved_docs)\n",
        "\n",
        "# Display the sources separately\n",
        "print(\"Source:\\n\", \"\\n\".join(retrieved_docs))\n",
        "print(\"Source metainfo:\\n\", \"\".join(sources))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62J73KZg-zGm",
        "outputId": "b4c82fe8-8ace-40fd-88b5-dd56b98ca5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            " <Response [200]>\n",
            "Sources:\n",
            " Sources:\n",
            "http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/sem02_japanese.html\n",
            "http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/sem02_japanese.html\n",
            "http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/sem02_japanese.html\n",
            "http://www.visual-memory.co.uk/daniel/Documents/S4B/japanese/sem02_japanese.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "tu6me_IPABbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://github.com/huggingface/cookbook/tree/main/notebooks\n",
        "- https://python.langchain.com/v0.2/docs/tutorials/rag/#retrieval-and-generation\n",
        "- [生成AIの新展開！？―学術研究支援用ボットを作ってみた。その(1)](https://digitalnagasaki.hatenablog.com/entry/2024/10/22/101846)"
      ],
      "metadata": {
        "id": "U9Nc7o68AIhz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLQv9qDz_di3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}